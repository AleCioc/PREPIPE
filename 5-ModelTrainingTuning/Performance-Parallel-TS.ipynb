{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to compute the Grid Search by using Apache Spark validating D1 with the Time Series Cross Validation and D2 with Hold Out. \n",
    "\n",
    "This notebook implements the Time Series Cross Validation + Hold Out technique. As input it requires a tabular dataset like 1-SignalSelection/dataset/All.pkl.\n",
    "\n",
    "Input: \n",
    "- < Classifier >: the classifier to tune {tree, forest, svm, mlp}\n",
    "- < OptimizationStep >: which step should be tested. The process finds the dataset in the dataset folder of that step\n",
    "- < Selected>: the dataset to test  \n",
    "- < D1DatasetSize >: The portion of dataset assigned to D1\n",
    "- < DeltaT >: The window size default = 60 minutes\n",
    "    \n",
    "Output: \n",
    "\n",
    "- < \"gridresult/\"+OptimizationStep > save a file with the the classification performance of each hyperparamter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import math  \n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os \n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "broadcastVar = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(p,r):\n",
    "    return 0 if p == 0 or r == 0 else (2 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "def getPerformance(y_true, y_pred):\n",
    "\n",
    "    C = confusion_matrix (y_true, y_pred) \n",
    "\n",
    "    labels = [ str(s) for s in sorted(list(set(y_true)))]\n",
    "\n",
    "\n",
    "    accuracy = round(float(C.diagonal().sum()) /C.sum(),4)\n",
    "\n",
    "    stats = {\"green\" : {}, \"yellow\" : {}, \"red\":{}}\n",
    "    f1 = {}\n",
    "    tex = []\n",
    "    p = 0\n",
    "    r = 0\n",
    "    for i,label in enumerate(labels): \n",
    "        try:\n",
    "            p = float(C[i][i]) / C.transpose()[i].sum()\n",
    "            if math.isnan(p):\n",
    "                p=0\n",
    "        except:\n",
    "            p = 0\n",
    "        try:\n",
    "            r = float(C[i][i]) / C[i].sum()\n",
    "            if math.isnan(r):\n",
    "                r=0\n",
    "        except:\n",
    "            r = 0\n",
    "\n",
    "\n",
    "        stats[label]['precision'] = round(p,4)\n",
    "        stats[label]['recall'] = round(r,4)\n",
    "        stats[label][\"f1-score\"] = round(f1score(p,r),4)\n",
    "    stats[\"accuracy\"] = accuracy\n",
    "\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def TimeSeriesValidationPerformance(clf, D1Dataset, TesingWindow, userScale = True):\n",
    "\n",
    "    buffer = 100\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    y_pred_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    train_index = []  \n",
    "\n",
    "    \n",
    "    features = list(D1Dataset.columns)\n",
    "    features.remove(\"ExpID\")\n",
    "    features.remove(\"Label\")\n",
    "\n",
    "    X = np.array(D1Dataset[features])\n",
    "    y = np.array(D1Dataset['Label'])    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for count in range(0,len(X)):\n",
    "        train_index.append(count)\n",
    "        if(len(train_index)==buffer): \n",
    "            \n",
    "            test_index = [i for i in range(count+1, min(count+TesingWindow+1,len(X)))]\n",
    "            if(len(test_index)==TesingWindow):\n",
    "                buffer+=TesingWindow    \n",
    "\n",
    "                X_train = X[train_index]\n",
    "                if userScale == True:\n",
    "                    scaler.fit(X_train)\n",
    "                    X_train = scaler.transform(X_train)\n",
    "\n",
    "                clf.fit(X_train, y[train_index])\n",
    "\n",
    "                X_test = X[test_index]\n",
    "                if userScale == True:\n",
    "                    X_test = scaler.transform(X_test)\n",
    "\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_true = y[test_index]\n",
    "\n",
    "                y_pred_all += list(y_pred)\n",
    "                y_true_all += list(y_true)\n",
    "\n",
    "    stats = getPerformance(y_true_all, y_pred_all)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def HoldOutPerformance(clf, D1Dataset, D2Dataset, userScale = True):\n",
    "\n",
    "\n",
    "    features = list(D1Dataset.columns)\n",
    "    features.remove(\"ExpID\")\n",
    "    features.remove(\"Label\")\n",
    "\n",
    "    X_train = np.array(D1Dataset[features])\n",
    "    y_train = list(D1Dataset['Label'])\n",
    "\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    if userScale == True:\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    X_test = np.array(D2Dataset[features])\n",
    "    y_test = np.array(D2Dataset['Label'])\n",
    "    \n",
    "    \n",
    "    if userScale == True:\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_true = y_test\n",
    "\n",
    "    stats = getPerformance(y_true, y_pred)\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(args):\n",
    "        \n",
    "    (params_id, clf,classifier,TesingWindow,D1DatasetSize) = args\n",
    "\n",
    "    Scaler = False\n",
    "    if(classifier==\"svm\" or classifier==\"mlp\"): Scaler=True\n",
    "    \n",
    "    Dataset   = broadcastVar.value\n",
    "    D1Dataset = Dataset.iloc[:D1DatasetSize]\n",
    "    D2Dataset = Dataset.iloc[D1DatasetSize:]\n",
    "\n",
    "    TimeValidation = TimeSeriesValidationPerformance(clf, D1Dataset, TesingWindow, Scaler)\n",
    "    HoldValidation = HoldOutPerformance(clf, D1Dataset, D2Dataset, Scaler)\n",
    "    \n",
    "    stats = {\"Time\":TimeValidation,\"Hold\":HoldValidation}\n",
    "     \n",
    "    return (params_id,(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMLPGrid(N):\n",
    "\n",
    "    m = 3 #number of classes\n",
    "    first  = np.sqrt((m+2)*N)+2*np.sqrt(N/(m+2))\n",
    "    second = m*np.sqrt(N/(m+2))\n",
    "\n",
    "    hidden_layer =  set([int(first*i)  for i in [1]] +  [int(second*i) for i in [1]]) #0.1, 0.5,0.75,0.85,\n",
    "\n",
    "    i=6\n",
    "    while len(hidden_layer) < 10:\n",
    "\n",
    "        hidden_layer.add(int(first*(1-float(i)/10)))\n",
    "        if(len(hidden_layer) < 10):\n",
    "            hidden_layer.add(int(second*(1-float(i)/10)))\n",
    "        i+=1    \n",
    "        if(i>=10): \n",
    "            print(\"ERORR NUMBER OF NODES CANNOT BE 0 OR NEGATIVE\")\n",
    "            exit(0)\n",
    "    \n",
    "    activation_functions = ['logistic']#, 'tanh']\n",
    "    tol_array = [1e-4]\n",
    "    random.seed(0)\n",
    "    random_states = [random.randint(0, 2 ** 32 - 1) for i in range(0, 100)]\n",
    "\n",
    "    tuple_layer = []\n",
    "    # create al possible combination\n",
    "    for i in hidden_layer:\n",
    "        for j in hidden_layer:\n",
    "            tuple_layer.append((i, j))\n",
    "\n",
    "    conf_list = []\n",
    "    for tuple in tuple_layer:\n",
    "        for activation_function in activation_functions:\n",
    "            for tol in tol_array:\n",
    "                for rs in random_states:\n",
    "                    conf = dict(hidden_layer_sizes=tuple, max_iter=5000, tol=tol, solver='adam', \\\n",
    "                                activation=activation_function, random_state=rs)\n",
    "                    conf_list.append(conf)\n",
    "\n",
    "    return conf_list\n",
    "\n",
    "def loadConfig(classifier, N = 300):\n",
    "    \n",
    "    if(classifier == \"mlp\"): return createMLPGrid(N)\n",
    "\n",
    "    configs = json.load(open(\"../classes/parameters/grid_\"+classifier+\".json\"))\n",
    "    \n",
    "    return configs \n",
    "\n",
    "def main(Classifier,OptimizationStep,Selected,TesingWindow,DeltaT,D1DatasetSize):\n",
    "\n",
    "\n",
    "    Dataset = pickle.load(open(\"../\"+OptimizationStep+\"/dataset/\"+Selected+\".pkl\",\"rb\"))\n",
    "    \n",
    "    WinExperiment = int(float(62)/float(DeltaT))\n",
    "    D1DatasetSize *=WinExperiment\n",
    "    TesingWindow *=WinExperiment\n",
    "\n",
    "    global broadcastVar\n",
    "    broadcastVar = sc.broadcast(Dataset)\n",
    "\n",
    "    configurations = loadConfig(Classifier,D1DatasetSize)\n",
    "    \n",
    "    clfs = []\n",
    "    ID_Params = {}\n",
    "    ID=0\n",
    "    for params in configurations:\n",
    "        clf = \"\"\n",
    "        if(Classifier==\"forest\"):\n",
    "            clf = RandomForestClassifier(**params)\n",
    "        if(Classifier==\"tree\"):\n",
    "            clf = DecisionTreeClassifier(**params)\n",
    "        if(Classifier==\"svm\"):\n",
    "            clf = SVC(**params)\n",
    "        if(Classifier==\"mlp\"):\n",
    "            clf = MLPClassifier(**params)\n",
    "        clfs.append((ID,clf,Classifier,TesingWindow,D1DatasetSize))\n",
    "        ID_Params[ID] = params\n",
    "        ID+=1\n",
    "\n",
    "    \n",
    "    print(\"Run Configuration:\")\n",
    "    print(\" Step:\",OptimizationStep)\n",
    "    print(\" Step configuration:\",Selected)\n",
    "    print(\" Classifier:\",Classifier)\n",
    "    print(\" Hyperparameters configurations:\",len(clfs))\n",
    "    print(\" D1 Size:\",D1DatasetSize)    \n",
    "    print(\" Window Shift:\",TesingWindow)    \n",
    "    \n",
    "    \n",
    "    bs_rdd = sc.parallelize(clfs, numSlices=len(clfs))   \n",
    "    Performance = bs_rdd.map(run_classification).collectAsMap()\n",
    "    \n",
    "    if not os.path.exists('gridresult'):\n",
    "        os.makedirs('gridresult')    \n",
    "\n",
    "    if not os.path.exists('gridresult/'+OptimizationStep):\n",
    "        os.makedirs('gridresult/'+OptimizationStep)       \n",
    "    \n",
    "    fout = open(\"gridresult/\"+OptimizationStep+\"/TS_\"+Selected+\"-\"+Classifier+\"_\"+str(TesingWindow)+\".csv\",\"w\")\n",
    "    fout.write(\"ID;Conf;D1G-P;D1G-R;D1G-F1;D1Y-P;D1Y-R;D1Y-F1;D1R-P;D1R-R;D1R-F1;D1Accuracy;D2G-P;D2G-R;D2G-F1;D2Y-P;D2Y-R;D2Y-F1;D2R-P;D2R-R;D2R-F1;D2Accuracy\\n\")\n",
    "\n",
    "    \n",
    "    for ID in range(len(Performance)):\n",
    "        PerformanceI = Performance[ID]\n",
    "        Params = ID_Params[ID]\n",
    "        \n",
    "        StrOut = str(ID)+\";\"+str(Params)+\";\"\n",
    "        for Validation in [\"Time\",\"Hold\"]:\n",
    "            for label in ['green','yellow','red']:\n",
    "                for measure in ['precision','recall','f1-score']:\n",
    "                    StrOut +=\"%.4f;\"%PerformanceI[Validation][label][measure]\n",
    "            StrOut +=\"%.4f;\"%PerformanceI[Validation]['accuracy']\n",
    "        \n",
    "        fout.write(StrOut[0:-1]+\"\\n\")\n",
    "        \n",
    "    fout.close()\n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start  15:41 \n",
      "Run Configuration:\n",
      " Step: 2-Windowing\n",
      " Step configuration: 60\n",
      " Classifier: tree\n",
      " Hyperparameters configurations: 1344\n",
      " D1 Size: 300\n",
      " Window Shift: 3\n"
     ]
    }
   ],
   "source": [
    "Classifier = \"tree\"\n",
    "OptimizationStep = \"2-Windowing\"\n",
    "Selected = \"60\"\n",
    "\n",
    "D1DatasetSize = 300\n",
    "DeltaT = 60\n",
    "TesingWindow = 3\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start %s\"%(time.strftime(\" %R \")))\n",
    "main(Classifier,OptimizationStep,Selected,TesingWindow,DeltaT,D1DatasetSize)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
